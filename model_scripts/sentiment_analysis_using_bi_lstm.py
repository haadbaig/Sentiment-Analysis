# -*- coding: utf-8 -*-
"""Sentiment Analysis using Bi-LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUkHzUbGit91subYgpoBk7RjkzJRtijG
"""

from keras.datasets import imdb
import pandas as pd
import numpy as np
from keras import backend as K
from keras.layers import LSTM, Activation, Dropout, Lambda, Dense, Input, SimpleRNN, GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Bidirectional
from keras.layers.embeddings import Embedding
from keras.models import Model, Sequential
import string
import re
from keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
from keras.preprocessing.sequence import pad_sequences
import keras
from sklearn.model_selection import train_test_split
import tensorflow as tf

# from google.colab import drive
# drive.mount('/content/drive')

URL = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file(fname="aclImdb_v1.tar.gz", 
                                  origin=URL,
                                  untar=True,
                                  cache_dir='.',
                                  cache_subdir='')

# data = pd.read_csv('/content/drive/MyDrive/assignment2/sarcasm_dataset.csv')
# data["tweet"]

import os
import shutil
# Create main directory path ("/aclImdb")
main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
# Create sub directory path ("/aclImdb/train")
train_dir = os.path.join(main_dir, 'train')
# Remove unsup folder since this is a supervised learning task
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)
# View the final train folder
# print(os.listdir(train_dir))

trainn = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=30000, validation_split=0.2, 
    subset='training', seed=123)
testt = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=30000, validation_split=0.2, 
    subset='validation', seed=123)

for i in trainn.take(1):
  train_feat = i[0].numpy()
  train_lab = i[1].numpy()

train = pd.DataFrame([train_feat, train_lab]).T
train.columns = ['text', 'polarity']
train['text'] = train['text'].str.decode("utf-8")
train.head()

train['text'][3]

for j in testt.take(1):
  test_feat = j[0].numpy()
  test_lab = j[1].numpy()

test = pd.DataFrame([test_feat, test_lab]).T
test.columns = ['text', 'polarity']
test['text'] = test['text'].str.decode("utf-8")
test.head()

!pip install tweet-preprocessor
!pip install pandas==1.2.5 --use-deprecated=legacy-resolver

import nltk
from nltk import word_tokenize, FreqDist
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.tokenize import TweetTokenizer

train["text"] = train['text'].astype(str).str.replace('\d+', '')
lower_text = train["text"].str.lower()
lower_text
train['polarity'].value_counts()

stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", 
             "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during",
             "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", 
             "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into",
             "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or",
             "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", 
             "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's",
             "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up",
             "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's",
             "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've",
             "your", "yours", "yourself", "yourselves" ]

def remove_stopwords(data):
  return data.apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))

def remove_tags(string):
    result = re.sub('<.*?>','',string)
    return result

pre_tweet = remove_stopwords(lower_text)
pre_tweet = pre_tweet.apply(lambda cw : remove_tags(cw))
pre_tweet = pre_tweet.str.replace('[{}]'.format(string.punctuation), ' ')

pre_tweet

import re
def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642" 
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, '', data)

def remove_mentions_tags(data):
  return re.sub(r'(@|https?)\S+|#', '', data)

pre_tweet = pre_tweet.apply(lambda row : remove_emojis(row))
pre_tweet = pre_tweet.apply(lambda row : remove_mentions_tags(row))
pre_tweet

# pre_tweets = preprocess_data(data['tweet'])
len(pre_tweet)

Y = pd.get_dummies(train['polarity']).values

vocab_size = 10000
embedding_dim=16
max_length = 120
trunc_type= 'post'

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=2000, split=' ')
tokenizer.fit_on_texts(pre_tweet.values)
tokenz = tokenizer.texts_to_sequences(pre_tweet.values)
word_to_index = tokenizer.word_index
X = pad_sequences(tokenz)
X.shape

import pickle
from google.colab import files

filename = 'tokenizer.pkl'
pickle.dump(tokenizer, open(filename, 'wb'))

files.download(filename)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

HIDDEN_SIZE = 128
embed_dim = 128

model = Sequential()
model.add(Embedding(2000, embed_dim, input_length = X.shape[1]))
# model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(X.shape[1],1), activation="sigmoid")))
# model.add(Dropout(0.7))
model.add(Bidirectional(LSTM(HIDDEN_SIZE, input_shape=(X.shape[1],1), activation="sigmoid")))
model.add(Dropout(0.7))
model.add(Dense(2, activation="softmax"))

model.summary()
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = 
              ['accuracy', f1_m, precision_m, recall_m])

model.fit(X_train, y_train, epochs =

token = tokenizer.texts_to_sequences(["Even though the name was good, but the movie was pretty amazing."])
word_to_index = tokenizer.word_index
xy = model.predict(pad_sequences(token))
xy[0]

import pickle
from google.colab import files

filename = 'finalized_model.pkl'
pickle.dump(model, open(filename, 'wb'))

files.download(filename)

HIDDEN_SIZE = 128
embed_dim = 128

model_lstm = Sequential()
model_lstm.add(Embedding(2000, embed_dim, input_length = X.shape[1]))
# model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(X.shape[1],1), activation="sigmoid")))
# model.add(Dropout(0.7))
model_lstm.add(LSTM(HIDDEN_SIZE, input_shape=(X.shape[1],1), activation="sigmoid"))
model_lstm.add(Dropout(0.7))
model_lstm.add(Dense(2, activation="softmax"))

model_lstm.summary()
model_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = 
              ['accuracy', f1_m, precision_m, recall_m])

model_lstm.fit(X_train, y_train, epochs = 5, verbose = 2)

model.predict(X_test)

def read_glove_vector(glove_vec):
  with open(glove_vec, 'r', encoding='UTF-8') as f:
    words = set()
    word_to_vec_map = {}
    for line in f:
      w_line = line.split()
      curr_word = w_line[0]
      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)
  return word_to_vec_map

  
word_to_vec_map = read_glove_vector('/content/drive/MyDrive/Sentiment Analysis/glove.6B.50d.txt')

embed_vector_len = word_to_vec_map['moon'].shape[0]

emb_matrix = np.zeros((2000, embed_vector_len))

for word, index in word_to_index.items():
  embedding_vector = word_to_vec_map.get(word)
  if embedding_vector is not None and index < 2000:
    emb_matrix[index, :] = embedding_vector

embedding_layer = Embedding(input_dim=2000, output_dim=embed_vector_len, input_length=32, weights = [emb_matrix], trainable=False)

HIDDEN_SIZE = 128
embed_dim = 128

model = Sequential()
# model.add(Embedding(2000, embed_dim, input_length = X.shape[1]))
model.add(embedding_layer)
model.add(Bidirectional(LSTM(HIDDEN_SIZE, input_shape=(X.shape[1],1), activation="sigmoid")))
model.add(Dropout(0.7))
model.add(Dense(2, activation="softmax"))

model.summary()
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = 
              ['accuracy', f1_m, precision_m, recall_m])

model.fit(X_train, y_train, epochs = 25, verbose = 2)

import gensim.downloader as api
from gensim.models import KeyedVectors
word2vec = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/assignment2/GoogleNews-vectors-negative300.bin", \
        binary=True)

vector_size = 300
gensim_weight_matrix = np.zeros((300 ,vector_size))
gensim_weight_matrix.shape
for word, index in tokenizer.word_index.items():
    if index < 300: # since index starts with zero 
        if word in word2vec.wv.vocab:
            gensim_weight_matrix[index] = word2vec[word]
        else:
            gensim_weight_matrix[index] = np.zeros(300)

"""### Model Function"""

def BiLSTM(embed_layer):
  model = Sequential()
  # model.add(Embedding(2000, embed_dim, input_length = X.shape[1]))
  model.add(embed_layer)
  model.add(Dropout(0.7))
  model.add(Bidirectional(LSTM(HIDDEN_SIZE, input_shape=(X.shape[1],1), activation="sigmoid")))
  model.add(Dense(2, activation="softmax"))
  return model

embed = Embedding(300,300,input_length = X.shape[1],
  weights = [gensim_weight_matrix],trainable = False)
model = BiLSTM(embed)
model.summary()
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = 
                ['accuracy', f1_m, precision_m, recall_m])

model.fit(X_train, y_train, batch_size=64, epochs = 25)

# import tensorflow.compat.v1 as tf
# tf.disable_v2_behavior()
# import tensorflow_hub as hub

# module = hub.Module("https://tfhub.dev/google/elmo/3",name='elmo_module')
# ELMoEmbedding = elmo(tf.reshape(tf.cast(input_text, tf.string), [-1]), signature="default", as_dict=True)["elmo"]
# embed_elmo = embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ), name="Elmo_Embedding")(input_layer)

types  = ['sarcasm', 'irony', 'satire', 'understatement', 'overstatement', 'rhetorical_question']
def function(row):
    if row.sarcasm == 1.0:
        return 1
    elif row.irony == 1.0:
        return 2
    elif row.satire == 1.0:
        return 3
    elif row.understatement == 1.0:
        return 4
    elif row.overstatement == 1.0:
        return 5
    elif row.rhetorical_question == 1.0:
        return 6

data['type'] = data[types].apply(function, axis=1)
Y = pd.get_dummies(data['sarcastic']).values

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)

HIDDEN_SIZE = 128
embed_dim = 128

model = Sequential()
model.add(Embedding(2000, embed_dim, input_length = X.shape[1]))
model.add(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(X.shape[1],1), activation="sigmoid"))
model.add(Dropout(0.3))
model.add(LSTM(HIDDEN_SIZE, input_shape=(X.shape[1],1), activation="sigmoid"))
model.add(Dropout(0.3))
model.add(Dense(2, activation="softmax"))

# model.summary()
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = 
              ['accuracy', f1_m, precision_m, recall_m])

model.fit(X_train, y_train, epochs = 25, verbose = 2)

!pip install -U pip setuptools wheel
!pip install -U spacy
!python -m spacy download en_core_web_sm

import spacy
from spacy import displacy

# Load the language model
nlp = spacy.load("en_core_web_sm")

sentence = 'I thought he was a bad person but I was wrong'

# nlp function returns an object with individual token information, 
# linguistic features and relationships

dg = []

for sen in train['text']:
  doc = nlp(sen)
  temp = [sen, doc]
  dg.append(temp)

print ("{:<15} | {:<8} | {:<15} | {:<20}".format('Token','Relation','Head', 'Children'))
print ("-" * 70)

for token in doc:
    # Print the token, dependency nature, head and all dependents of the token
    print ("{:<15} | {:<8} | {:<15} | {:<20}"
    .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children])))
  
    # Use displayCy to visualize the dependency 
displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})